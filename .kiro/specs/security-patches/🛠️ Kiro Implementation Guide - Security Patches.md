# üõ†Ô∏è Kiro Implementation Guide - Security Patches

## üéØ **CRITICAL: EXACT CODE CHANGES REQUIRED**

This document provides the **specific code implementations** that Kiro must apply to fix all critical security vulnerabilities. Each section includes exact file paths, line numbers, and replacement code.

---

## üö® **PRIORITY 1: AUTHENTICATION BYPASS FIX**

### **File: `server/routes/auth.py`**

**CRITICAL CHANGE - Line 142-150**

**REMOVE THIS DANGEROUS CODE:**
```python
# TEMPORARY WORKAROUND: Return a mock user profile for any token
# This allows the frontend to work without a valid token
# In a production environment, you would validate the token properly

# Create a mock user profile that exactly matches the UserProfile interface in App.tsx
# The frontend expects these exact fields with these exact types
return {
    "id": "mock-user-id",
    "email": "rekonnlabs@gmail.com",
    "subscription_tier": "pro",  # Must be one of: 'free', 'pro', 'pro_byok'
    "usage_count": 0,
    "usage_limit": 10000
}
```

**REPLACE WITH THIS SECURE CODE:**
```python
@router.get("/profile")
async def get_profile(authorization: str = Header(None)):
    """Get user profile and tier information with proper authentication"""
    try:
        if not authorization or not authorization.startswith('Bearer '):
            raise HTTPException(status_code=401, detail="Missing or invalid authorization header")
        
        token = authorization.split(' ')[1]
        
        # CRITICAL: Validate token with Supabase and check expiry
        try:
            user_response = supabase.auth.get_user(token)
            if not user_response.user:
                raise HTTPException(status_code=401, detail="Invalid or expired token")
            
            # Check if token is expired (Requirement 1.6)
            if await is_token_expired(token):
                logger.warning(f"Expired token used by user: {user_response.user.id}")
                raise HTTPException(status_code=401, detail="Token expired - please log in again")
                
        except Exception as e:
            logger.error(f"Token validation failed: {e}")
            raise HTTPException(status_code=401, detail="Authentication failed")
        
        user_id = user_response.user.id
        
        # Get user profile from database
        user_data = supabase.table('users').select('*').eq('id', user_id).execute()
        if not user_data.data:
            # Create user profile if it doesn't exist (for new users)
            new_user = {
                "id": user_id,
                "email": user_response.user.email,
                "plan": "free",
                "full_name": user_response.user.email.split('@')[0],
                "last_login": datetime.utcnow().isoformat()
            }
            supabase.table('users').insert(new_user).execute()
            user_profile = new_user
        else:
            user_profile = user_data.data[0]
            # Update last login timestamp
            supabase.table('users').update({
                "last_login": datetime.utcnow().isoformat()
            }).eq('id', user_id).execute()
        
        # Get current usage for this month
        current_usage = await get_user_usage(user_id)
        tier_limits = TIER_LIMITS.get(user_profile['plan'], TIER_LIMITS['free'])
        
        return {
            "id": user_profile['id'],
            "email": user_profile['email'],
            "subscription_tier": user_profile['plan'],
            "usage_count": current_usage,
            "usage_limit": tier_limits.max_llm_calls if tier_limits.max_llm_calls != -1 else 999999
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Profile error: {e}")
        raise HTTPException(status_code=500, detail="Failed to get profile")

async def is_token_expired(token: str) -> bool:
    """Check if token is expired (Requirement 1.6)"""
    try:
        # Supabase tokens have expiry built-in, but we can add additional checks
        user_response = supabase.auth.get_user(token)
        if not user_response.user:
            return True
        
        # Additional expiry logic can be added here if needed
        # For now, rely on Supabase's built-in token expiry
        return False
        
    except Exception:
        return True

async def invalidate_user_session(user_id: str) -> bool:
    """Invalidate user session (Requirement 1.6)"""
    try:
        # Log session invalidation for audit
        await log_audit_action(
            admin_id=None,  # System action
            target_user_id=user_id,
            action="session_invalidated",
            metadata={"reason": "security_patch", "timestamp": datetime.utcnow().isoformat()}
        )
        return True
    except Exception as e:
        logger.error(f"Failed to invalidate session: {e}")
        return False

async def get_user_usage(user_id: str) -> int:
    """Get current month usage for user"""
    try:
        current_month = datetime.utcnow().strftime('%Y-%m')
        usage_data = supabase.table('usage_logs').select('*').eq('user_id', user_id).eq('action', 'chat').gte('created_at', f'{current_month}-01').execute()
        return len(usage_data.data)
    except Exception as e:
        logger.error(f"Usage retrieval error: {e}")
        return 0
```

**ADD THESE IMPORTS at the top of auth.py:**
```python
from datetime import datetime
import logging

logger = logging.getLogger(__name__)
```

---

## üîí **PRIORITY 2: USER ISOLATION FIX**

### **File: `server/vector_store.py`**

**CRITICAL CHANGE - Line 45-60**

**MODIFY the ChromaVectorStore __init__ method:**
```python
class ChromaVectorStore:
    """Cloud-native vector store using Chroma Cloud with user isolation"""
    
    def __init__(self, user_id: str = None, collection_name: str = None):
        # CRITICAL: Implement per-user isolation
        if user_id:
            self.collection_name = f"user_{user_id}_docs"
            self.user_id = user_id
            logger.info(f"üîí Initializing user-specific collection: {self.collection_name}")
        else:
            self.collection_name = collection_name or "briefly_cloud_docs"
            self.user_id = None
            logger.warning("‚ö†Ô∏è Initializing shared collection - ensure this is intentional")
        
        self.client = None
        self.collection = None
        self.cache = FileCache()
        self._initialize_client()
    
    def validate_user_access(self, user_id: str) -> bool:
        """CRITICAL: Ensure user can only access their own collection"""
        if self.user_id and self.user_id != user_id:
            logger.error(f"üö® Access violation: User {user_id} attempted to access collection for user {self.user_id}")
            raise HTTPException(status_code=403, detail="Access denied to user data")
        return True
    
    def is_available(self) -> bool:
        """Check if ChromaDB is available and connected"""
        return self.client is not None and self.collection is not None
```

**UPDATE the get_vector_store function (around line 300):**
```python
def get_vector_store(user_id: str = None):
    """Get vector store instance with user isolation"""
    if user_id:
        return ChromaVectorStore(user_id=user_id)
    else:
        logger.warning("‚ö†Ô∏è Creating vector store without user_id - ensure this is for admin operations only")
        return ChromaVectorStore()
```

---

## üìä **PRIORITY 3: USAGE TRACKING IMPLEMENTATION**

### **File: `server/services/usage_tracking.py` (NEW FILE)**

**CREATE THIS NEW FILE:**
```python
"""
Usage tracking service for Briefly Cloud
Handles usage logging and tier limit enforcement
"""

import os
import logging
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from fastapi import HTTPException
from supabase import create_client, Client

# Configure logging
logger = logging.getLogger(__name__)

# Initialize Supabase client
supabase_url = os.getenv("SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_ANON_KEY")
supabase: Client = create_client(supabase_url, supabase_key)

# Tier limits configuration
TIER_LIMITS = {
    "free": {
        "max_files": 10,
        "max_llm_calls": 100,
        "max_file_size_mb": 10,
        "features": ["basic_chat", "pdf_upload"],
        "rate_limit_per_minute": 10
    },
    "pro": {
        "max_files": 1000,
        "max_llm_calls": 10000,
        "max_file_size_mb": 100,
        "features": ["advanced_chat", "all_formats", "priority_support"],
        "rate_limit_per_minute": 60
    },
    "pro_byok": {
        "max_files": 10000,
        "max_llm_calls": -1,  # Unlimited with own API key
        "max_file_size_mb": 500,
        "features": ["advanced_chat", "all_formats", "priority_support", "own_api_key"],
        "rate_limit_per_minute": 120
    }
}

async def log_usage(user_id: str, action: str, metadata: Dict[str, Any] = None, request=None):
    """Log user action for usage tracking"""
    try:
        usage_record = {
            "user_id": user_id,
            "action": action,
            "metadata": metadata or {},
            "created_at": datetime.utcnow().isoformat(),
            "ip_address": getattr(request.client, 'host', None) if request else None,
            "user_agent": request.headers.get('user-agent') if request else None
        }
        
        result = supabase.table('usage_logs').insert(usage_record).execute()
        logger.info(f"üìä Logged usage: {user_id} - {action}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Failed to log usage: {e}")
        return False

async def check_usage_limits(user_id: str, action: str) -> bool:
    """CRITICAL: Check if user can perform action based on tier limits"""
    try:
        # Get user profile
        user_data = supabase.table('users').select('*').eq('id', user_id).execute()
        if not user_data.data:
            logger.error(f"User not found: {user_id}")
            return False
        
        user = user_data.data[0]
        tier = user.get('plan', 'free')
        tier_limits = TIER_LIMITS.get(tier, TIER_LIMITS['free'])
        
        if action == 'chat':
            max_calls = tier_limits['max_llm_calls']
            if max_calls == -1:  # Unlimited for BYOK
                return True
            
            # Get current month usage
            current_month = datetime.utcnow().strftime('%Y-%m')
            usage_data = supabase.table('usage_logs').select('*').eq('user_id', user_id).eq('action', 'chat').gte('created_at', f'{current_month}-01').execute()
            current_usage = len(usage_data.data)
            
            if current_usage >= max_calls:
                logger.warning(f"üö´ Chat limit exceeded for user {user_id}: {current_usage}/{max_calls}")
                return False
            
            return True
        
        elif action == 'document_upload':
            max_files = tier_limits['max_files']
            
            # Get current document count (excluding deleted)
            doc_count_data = supabase.table('user_documents').select('*').eq('user_id', user_id).is_('deleted_at', 'null').execute()
            current_docs = len(doc_count_data.data)
            
            if current_docs >= max_files:
                logger.warning(f"üö´ Document limit exceeded for user {user_id}: {current_docs}/{max_files}")
                return False
            
            return True
        
        # Default to allow if action not recognized
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error checking usage limits: {e}")
        return False

async def get_user_usage_stats(user_id: str) -> Dict[str, Any]:
    """Get comprehensive usage statistics for user"""
    try:
        current_month = datetime.utcnow().strftime('%Y-%m')
        
        # Get chat usage
        chat_usage = supabase.table('usage_logs').select('*').eq('user_id', user_id).eq('action', 'chat').gte('created_at', f'{current_month}-01').execute()
        
        # Get document count
        doc_count = supabase.table('user_documents').select('*').eq('user_id', user_id).is_('deleted_at', 'null').execute()
        
        # Get user tier
        user_data = supabase.table('users').select('*').eq('id', user_id).execute()
        tier = user_data.data[0].get('plan', 'free') if user_data.data else 'free'
        tier_limits = TIER_LIMITS.get(tier, TIER_LIMITS['free'])
        
        return {
            "chat_requests": len(chat_usage.data),
            "documents_stored": len(doc_count.data),
            "tier": tier,
            "limits": tier_limits,
            "period": current_month
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting usage stats: {e}")
        return {"error": str(e)}

async def log_audit_action(admin_id: str, target_user_id: str, action: str, old_value: dict = None, new_value: dict = None, metadata: dict = None):
    """Log administrative actions for audit trail (Requirement 7)"""
    try:
        audit_record = {
            "admin_id": admin_id,
            "target_user_id": target_user_id,
            "action": action,
            "old_value": old_value or {},
            "new_value": new_value or {},
            "metadata": metadata or {},
            "created_at": datetime.utcnow().isoformat()
        }
        
        result = supabase.table('audit_logs').insert(audit_record).execute()
        logger.info(f"üîç Audit logged: {action} by {admin_id} on user {target_user_id}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Failed to log audit action: {e}")
        return False

async def log_tier_change(admin_id: str, user_id: str, old_tier: str, new_tier: str):
    """Log tier changes for audit (Requirement 7.1)"""
    return await log_audit_action(
        admin_id=admin_id,
        target_user_id=user_id,
        action="tier_change",
        old_value={"tier": old_tier},
        new_value={"tier": new_tier},
        metadata={"change_type": "subscription_tier"}
    )

async def log_document_removal(admin_id: str, user_id: str, document_id: str, document_name: str):
    """Log document removal for audit (Requirement 7.2)"""
    return await log_audit_action(
        admin_id=admin_id,
        target_user_id=user_id,
        action="document_removed",
        old_value={"document_id": document_id, "document_name": document_name},
        metadata={"removal_type": "admin_action"}
    )

async def get_audit_logs(admin_id: str, target_user_id: str = None, limit: int = 100):
    """Get audit logs (Requirement 7.4 - only for authorized admins)"""
    try:
        # Verify admin permissions
        admin_data = supabase.table('users').select('*').eq('id', admin_id).execute()
        if not admin_data.data or admin_data.data[0].get('plan') != 'admin':
            raise HTTPException(status_code=403, detail="Admin access required")
        
        query = supabase.table('audit_logs').select('*').order('created_at', desc=True).limit(limit)
        
        if target_user_id:
            query = query.eq('target_user_id', target_user_id)
        
        result = query.execute()
        return result.data
        
    except Exception as e:
        logger.error(f"‚ùå Failed to get audit logs: {e}")
        return []

async def log_document_upload(user_id: str, file_name: str, file_size: int, file_type: str, storage_provider: str, chroma_collection: str):
    """Log document upload to user_documents table"""
    try:
        doc_record = {
            "user_id": user_id,
            "file_name": file_name,
            "file_size": file_size,
            "file_type": file_type,
            "storage_provider": storage_provider,
            "chroma_collection": chroma_collection,
            "indexed_at": datetime.utcnow().isoformat(),
            "metadata": {
                "upload_source": "api",
                "processing_status": "completed"
            }
        }
        
        result = supabase.table('user_documents').insert(doc_record).execute()
        logger.info(f"üìÑ Logged document upload: {user_id} - {file_name}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Failed to log document upload: {e}")
        return False
```

---

## ÔøΩ **PRIIORITY 4: RATE LIMITING IMPLEMENTATION**

### **File: `server/middleware/rate_limiting.py` (NEW FILE)**

**CREATE THIS NEW FILE:**
```python
"""
Rate limiting middleware for Briefly Cloud
Implements per-user rate limiting (Requirement 4.3)
"""

import os
import logging
from datetime import datetime, timedelta
from typing import Optional
from fastapi import HTTPException, Request
from supabase import create_client, Client

# Configure logging
logger = logging.getLogger(__name__)

# Initialize Supabase client
supabase_url = os.getenv("SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_ANON_KEY")
supabase: Client = create_client(supabase_url, supabase_key)

class UserRateLimiter:
    """Per-user rate limiting (Requirement 4.3)"""
    
    def __init__(self):
        self.default_limits = {
            'free': {'requests_per_minute': 10, 'requests_per_hour': 100},
            'pro': {'requests_per_minute': 60, 'requests_per_hour': 1000},
            'pro_byok': {'requests_per_minute': 120, 'requests_per_hour': 2000}
        }
    
    async def check_rate_limit(self, user_id: str, endpoint: str, user_tier: str = 'free') -> bool:
        """Check if user has exceeded rate limits"""
        try:
            limits = self.default_limits.get(user_tier, self.default_limits['free'])
            
            # Check minute window
            minute_window = datetime.utcnow().replace(second=0, microsecond=0)
            minute_key = f"{user_id}:{endpoint}:{minute_window.isoformat()}"
            
            # Get current minute requests
            minute_data = supabase.table('rate_limits').select('*').eq('user_id', user_id).eq('endpoint', endpoint).eq('window_start', minute_window.isoformat()).execute()
            
            minute_requests = minute_data.data[0]['requests_count'] if minute_data.data else 0
            
            if minute_requests >= limits['requests_per_minute']:
                logger.warning(f"üö´ Rate limit exceeded (minute): {user_id} - {minute_requests}/{limits['requests_per_minute']}")
                return False
            
            # Check hour window
            hour_window = datetime.utcnow().replace(minute=0, second=0, microsecond=0)
            hour_data = supabase.table('rate_limits').select('*').eq('user_id', user_id).eq('endpoint', endpoint).gte('window_start', hour_window.isoformat()).execute()
            
            hour_requests = sum(record['requests_count'] for record in hour_data.data)
            
            if hour_requests >= limits['requests_per_hour']:
                logger.warning(f"üö´ Rate limit exceeded (hour): {user_id} - {hour_requests}/{limits['requests_per_hour']}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Rate limit check error: {e}")
            return True  # Allow on error to prevent service disruption
    
    async def increment_usage(self, user_id: str, endpoint: str) -> bool:
        """Increment usage counter for user"""
        try:
            minute_window = datetime.utcnow().replace(second=0, microsecond=0)
            
            # Try to increment existing record
            existing = supabase.table('rate_limits').select('*').eq('user_id', user_id).eq('endpoint', endpoint).eq('window_start', minute_window.isoformat()).execute()
            
            if existing.data:
                # Update existing record
                supabase.table('rate_limits').update({
                    'requests_count': existing.data[0]['requests_count'] + 1
                }).eq('id', existing.data[0]['id']).execute()
            else:
                # Create new record
                supabase.table('rate_limits').insert({
                    'user_id': user_id,
                    'endpoint': endpoint,
                    'requests_count': 1,
                    'window_start': minute_window.isoformat()
                }).execute()
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to increment usage: {e}")
            return False

# Global rate limiter instance
rate_limiter = UserRateLimiter()

async def rate_limit_middleware(request: Request, user_id: str, user_tier: str = 'free'):
    """Rate limiting middleware for FastAPI"""
    endpoint = request.url.path
    
    # Check rate limit
    if not await rate_limiter.check_rate_limit(user_id, endpoint, user_tier):
        raise HTTPException(
            status_code=429,
            detail="Rate limit exceeded. Please slow down your requests.",
            headers={
                "X-RateLimit-Limit": str(rate_limiter.default_limits[user_tier]['requests_per_minute']),
                "X-RateLimit-Remaining": "0",
                "X-RateLimit-Reset": str(int((datetime.utcnow() + timedelta(minutes=1)).timestamp())),
                "Retry-After": "60"
            }
        )
    
    # Increment usage counter
    await rate_limiter.increment_usage(user_id, endpoint)
```

### **File: `server/main.py`**

**ADD RATE LIMITING MIDDLEWARE:**
```python
# Add this import at the top
from middleware.rate_limiting import rate_limit_middleware

# Add this middleware configuration after CORS
@app.middleware("http")
async def rate_limiting_middleware(request: Request, call_next):
    """Apply rate limiting to authenticated requests"""
    
    # Skip rate limiting for health checks and auth endpoints
    if request.url.path in ["/health", "/api/auth/login", "/api/auth/signup"]:
        response = await call_next(request)
        return response
    
    # Extract user info from authorization header
    auth_header = request.headers.get("authorization")
    if auth_header and auth_header.startswith("Bearer "):
        try:
            token = auth_header.split(" ")[1]
            user_response = supabase.auth.get_user(token)
            
            if user_response.user:
                user_id = user_response.user.id
                
                # Get user tier
                user_data = supabase.table('users').select('*').eq('id', user_id).execute()
                user_tier = user_data.data[0].get('plan', 'free') if user_data.data else 'free'
                
                # Apply rate limiting
                await rate_limit_middleware(request, user_id, user_tier)
        
        except Exception as e:
            logger.warning(f"Rate limiting middleware error: {e}")
            # Continue without rate limiting on error
    
    response = await call_next(request)
    return response
```

---

## üîÑ **PRIORITY 5: UPDATE ROUTE FILES**

### **File: `server/routes/chat.py`**

**ADD IMPORTS at the top:**
```python
from services.usage_tracking import check_usage_limits, log_usage, get_user_usage_stats
```

**MODIFY the chat_endpoint function (around line 85):**
```python
@router.post("/")
async def chat_endpoint(request: ChatRequest):
    """Main chat endpoint with context retrieval and LLM integration"""
    try:
        # CRITICAL: Check usage limits BEFORE processing
        if not await check_usage_limits(request.user_id, 'chat'):
            raise HTTPException(
                status_code=429, 
                detail="Usage limit exceeded for your tier. Please upgrade to continue.",
                headers={"X-Upgrade-URL": "/pricing"}
            )
        
        # Get user profile and tier info
        user_data = supabase.table('users').select('*').eq('id', request.user_id).execute()
        if not user_data.data:
            raise HTTPException(status_code=404, detail="User not found")
        
        user = user_data.data[0]
        tier = user.get('plan', 'free')
        
        # CRITICAL: Get user-specific vector store
        vector_store = get_vector_store(user_id=request.user_id)
        
        # Get relevant context from user's documents only
        context_chunks = await get_relevant_context(request.user_id, request.message)
        
        # Prepare conversation history
        conversation_history = []
        if request.conversation_id:
            conversation_history = await get_conversation_history(request.conversation_id)
        
        # Determine model to use
        model = determine_model(tier, user.get('api_key_hash'), request.model)
        
        # Generate response
        if request.stream:
            return StreamingResponse(
                stream_chat_response(
                    request.message,
                    context_chunks,
                    conversation_history,
                    model,
                    user
                ),
                media_type="text/plain"
            )
        else:
            response = await generate_chat_response(
                request.message,
                context_chunks,
                conversation_history,
                model,
                user
            )
            
            # CRITICAL: Log usage AFTER successful processing
            await log_usage(request.user_id, "chat", {
                "model": model,
                "message_length": len(request.message),
                "context_chunks": len(context_chunks)
            })
            
            return response
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail="Chat request failed")
```

### **File: `server/routes/embed.py`**

**ADD IMPORTS at the top:**
```python
from services.usage_tracking import check_usage_limits, log_usage, log_document_upload
```

**MODIFY the start_embedding function (around line 95):**
```python
@router.post("/start")
async def start_embedding(
    request: EmbedRequest,
    background_tasks: BackgroundTasks
):
    """Start document embedding process with user isolation"""
    try:
        # CRITICAL: Check document upload limits
        if not await check_usage_limits(request.user_id, 'document_upload'):
            raise HTTPException(
                status_code=429,
                detail="Document limit exceeded for your tier. Please upgrade to add more documents.",
                headers={"X-Upgrade-URL": "/pricing"}
            )
        
        # Validate user and storage connection
        token_data = supabase.table('oauth_tokens').select('*').eq('user_id', request.user_id).eq('provider', request.source).execute()
        
        if not token_data.data:
            raise HTTPException(status_code=404, detail=f"{request.source.title()} storage not connected")
        
        # CRITICAL: Get user-specific vector store
        vector_store = get_vector_store(user_id=request.user_id)
        
        # Create job log entry
        job_data = {
            "user_id": request.user_id,
            "job_type": "document_embedding",
            "status": "pending",
            "input_data": request.dict()
        }
        
        job_result = supabase.table('job_logs').insert(job_data).execute()
        job_id = job_result.data[0]['id']
        
        # Start background processing with user isolation
        background_tasks.add_task(
            process_documents_with_isolation,
            request.user_id,
            request.source,
            request.file_ids,
            job_id
        )
        
        return {
            "job_id": job_id,
            "status": "started",
            "message": "Document embedding started"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Embedding start error: {e}")
        raise HTTPException(status_code=500, detail="Failed to start embedding")

async def process_documents_with_isolation(user_id: str, source: str, file_ids: list, job_id: str):
    """Process documents with user isolation"""
    try:
        # CRITICAL: Use user-specific vector store
        vector_store = get_vector_store(user_id=user_id)
        
        # Process each document
        for file_id in file_ids:
            # Download and process file
            file_content = await download_file(user_id, source, file_id)
            
            # Extract text and create chunks
            chunks = await extract_and_chunk_text(file_content)
            
            # Add to user's vector store
            await vector_store.add_documents(chunks, user_id=user_id)
            
            # CRITICAL: Log document upload
            await log_document_upload(
                user_id=user_id,
                file_name=file_content['name'],
                file_size=file_content['size'],
                file_type=file_content['type'],
                storage_provider=source,
                chroma_collection=f"user_{user_id}_docs"
            )
        
        # Update job status
        supabase.table('job_logs').update({
            "status": "completed",
            "completed_at": datetime.utcnow().isoformat()
        }).eq('id', job_id).execute()
        
    except Exception as e:
        logger.error(f"Document processing error: {e}")
        supabase.table('job_logs').update({
            "status": "failed",
            "error_message": str(e)
        }).eq('id', job_id).execute()
```

---

## üóÑÔ∏è **PRIORITY 5: DATABASE MIGRATION**

### **File: `migrations/001_security_patches.sql` (NEW FILE)**

**CREATE THIS SQL FILE:**
```sql
-- Security patches migration for Briefly Cloud
-- Run this in Supabase SQL Editor

-- Create usage tracking table
CREATE TABLE IF NOT EXISTS usage_logs (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    action VARCHAR(50) NOT NULL,
    metadata JSONB DEFAULT '{}',
    ip_address INET,
    user_agent TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create user documents tracking with soft delete
CREATE TABLE IF NOT EXISTS user_documents (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    file_name VARCHAR(255) NOT NULL,
    file_size INTEGER,
    file_type VARCHAR(50),
    storage_provider VARCHAR(20),
    chroma_collection VARCHAR(100),
    indexed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    deleted_at TIMESTAMP WITH TIME ZONE NULL,
    metadata JSONB DEFAULT '{}'
);

-- Create audit logs for administrative actions
CREATE TABLE IF NOT EXISTS audit_logs (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    admin_id UUID REFERENCES auth.users(id),
    target_user_id UUID REFERENCES auth.users(id),
    action VARCHAR(100) NOT NULL,
    old_value JSONB,
    new_value JSONB,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create rate limiting state table
CREATE TABLE IF NOT EXISTS rate_limits (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    endpoint VARCHAR(100) NOT NULL,
    requests_count INTEGER DEFAULT 0,
    window_start TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(user_id, endpoint, window_start)
);

-- Create performance indexes
CREATE INDEX IF NOT EXISTS idx_usage_logs_user_id ON usage_logs(user_id);
CREATE INDEX IF NOT EXISTS idx_usage_logs_created_at ON usage_logs(created_at);
CREATE INDEX IF NOT EXISTS idx_usage_logs_action ON usage_logs(action);
CREATE INDEX IF NOT EXISTS idx_usage_logs_user_action ON usage_logs(user_id, action);

CREATE INDEX IF NOT EXISTS idx_user_documents_user_id ON user_documents(user_id);
CREATE INDEX IF NOT EXISTS idx_user_documents_deleted_at ON user_documents(deleted_at);
CREATE INDEX IF NOT EXISTS idx_user_documents_user_active ON user_documents(user_id) WHERE deleted_at IS NULL;

CREATE INDEX IF NOT EXISTS idx_audit_logs_target_user_id ON audit_logs(target_user_id);
CREATE INDEX IF NOT EXISTS idx_audit_logs_created_at ON audit_logs(created_at);

CREATE INDEX IF NOT EXISTS idx_rate_limits_user_endpoint ON rate_limits(user_id, endpoint);
CREATE INDEX IF NOT EXISTS idx_rate_limits_window_start ON rate_limits(window_start);

-- Add RLS (Row Level Security) policies for data isolation
ALTER TABLE usage_logs ENABLE ROW LEVEL SECURITY;
ALTER TABLE user_documents ENABLE ROW LEVEL SECURITY;
ALTER TABLE audit_logs ENABLE ROW LEVEL SECURITY;

-- Policy: Users can only see their own usage logs
CREATE POLICY "Users can view own usage logs" ON usage_logs
    FOR SELECT USING (auth.uid() = user_id);

-- Policy: Users can only see their own documents
CREATE POLICY "Users can view own documents" ON user_documents
    FOR SELECT USING (auth.uid() = user_id);

-- Policy: Only admins can view audit logs
CREATE POLICY "Admins can view audit logs" ON audit_logs
    FOR SELECT USING (
        EXISTS (
            SELECT 1 FROM users 
            WHERE id = auth.uid() 
            AND plan = 'admin'
        )
    );

-- Insert initial data if needed
INSERT INTO users (id, email, plan, full_name) 
VALUES ('00000000-0000-0000-0000-000000000000', 'admin@brieflycloud.com', 'admin', 'System Admin')
ON CONFLICT (id) DO NOTHING;
```

---

## ‚öôÔ∏è **PRIORITY 6: ENVIRONMENT CONFIGURATION**

### **File: `server/.env`**

**UPDATE THESE SPECIFIC LINES:**
```env
# CRITICAL: Change from development to production
DEBUG=False
ENVIRONMENT=production
LOG_LEVEL=WARNING

# CRITICAL: Update CORS for production domains (replace with your actual domains)
ALLOWED_ORIGINS=https://yourdomain.com,https://app.yourdomain.com

# CRITICAL: Add rate limiting configuration
RATE_LIMIT_PER_MINUTE=60
RATE_LIMIT_PER_HOUR=1000
RATE_LIMIT_BURST=10

# CRITICAL: Add security headers
SECURITY_HEADERS_ENABLED=True
HSTS_MAX_AGE=31536000
CONTENT_SECURITY_POLICY=default-src 'self'

# CRITICAL: Update ChromaDB URL (remove /v1 suffix)
CHROMA_CLOUD_URL=https://api.trychroma.com

# Add session security
SESSION_COOKIE_SECURE=True
SESSION_COOKIE_HTTPONLY=True
SESSION_COOKIE_SAMESITE=strict
```

---

## üß™ **PRIORITY 7: SECURITY TESTS & AUTOMATED REGRESSION**

### **File: `tests/test_security_patches.py` (NEW FILE)**

**CREATE THIS TEST FILE:**
```python
"""
Security patch validation tests
CRITICAL: These tests must pass before deployment
"""

import pytest
import asyncio
from fastapi.testclient import TestClient
from server.main import app

client = TestClient(app)

def test_authentication_bypass_removed():
    """CRITICAL: Verify mock user bypass is completely removed"""
    response = client.get("/api/auth/profile")
    assert response.status_code == 401
    assert "mock-user-id" not in response.text
    assert "rekonnlabs@gmail.com" not in response.text

def test_authentication_required_for_all_endpoints():
    """CRITICAL: Verify all protected endpoints require authentication"""
    protected_endpoints = [
        "/api/auth/profile",
        "/api/chat/",
        "/api/embed/start",
        "/api/storage/status"
    ]
    
    for endpoint in protected_endpoints:
        response = client.get(endpoint)
        assert response.status_code == 401, f"Endpoint {endpoint} should require authentication"

def test_user_isolation_enforced():
    """CRITICAL: Verify users cannot access other users' data"""
    # This test requires setting up test users
    # Implementation depends on your test setup
    pass

def test_usage_limits_enforced():
    """CRITICAL: Verify tier limits are actually enforced"""
    # This test requires setting up test users with different tiers
    # Implementation depends on your test setup
    pass

def test_rate_limiting_works():
    """CRITICAL: Verify rate limiting prevents abuse"""
    # Make multiple rapid requests
    responses = []
    for i in range(100):
        response = client.get("/api/health")
        responses.append(response.status_code)
    
    # Should eventually get rate limited
    assert 429 in responses, "Rate limiting should trigger with rapid requests"

def test_production_configuration():
    """CRITICAL: Verify production settings are applied"""
    import os
    assert os.getenv("DEBUG") == "False", "Debug mode should be disabled"
    assert os.getenv("ENVIRONMENT") == "production", "Environment should be production"

if __name__ == "__main__":
    pytest.main([__file__])
```

### **File: `.github/workflows/security-regression.yml` (NEW FILE)**

**CREATE AUTOMATED REGRESSION TESTS (Requirement 6.6):**
```yaml
name: Security Regression Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  security-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r server/requirements.txt
        pip install pytest pytest-asyncio
    
    - name: Run Security Tests
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        SUPABASE_SERVICE_ROLE: ${{ secrets.SUPABASE_SERVICE_ROLE }}
      run: |
        cd server
        python -m pytest tests/test_security_patches.py -v
    
    - name: Security Scan
      run: |
        pip install bandit safety
        bandit -r server/ -f json -o security-report.json || true
        safety check --json --output safety-report.json || true
    
    - name: Upload Security Reports
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          security-report.json
          safety-report.json

  integration-tests:
    runs-on: ubuntu-latest
    needs: security-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r server/requirements.txt
        pip install pytest pytest-asyncio httpx
    
    - name: Run Integration Tests
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        SUPABASE_SERVICE_ROLE: ${{ secrets.SUPABASE_SERVICE_ROLE }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        CHROMA_API_KEY: ${{ secrets.CHROMA_API_KEY }}
      run: |
        cd server
        python -m pytest tests/ -v --ignore=tests/test_security_patches.py

### **File: `tests/test_user_isolation.py` (NEW FILE)**

**CREATE USER ISOLATION TESTS (Requirement 2):**
```python
"""
User isolation tests - CRITICAL for data security
"""

import pytest
import asyncio
from server.vector_store import ChromaVectorStore, get_vector_store
from server.services.usage_tracking import log_usage, check_usage_limits

@pytest.mark.asyncio
async def test_user_collections_isolated():
    """Test that users get separate ChromaDB collections"""
    user1_id = "test-user-1"
    user2_id = "test-user-2"
    
    # Create vector stores for different users
    store1 = get_vector_store(user_id=user1_id)
    store2 = get_vector_store(user_id=user2_id)
    
    # Verify different collection names
    assert store1.collection_name == f"user_{user1_id}_docs"
    assert store2.collection_name == f"user_{user2_id}_docs"
    assert store1.collection_name != store2.collection_name

@pytest.mark.asyncio
async def test_cross_user_access_denied():
    """Test that users cannot access other users' data"""
    user1_id = "test-user-1"
    user2_id = "test-user-2"
    
    store1 = ChromaVectorStore(user_id=user1_id)
    
    # Attempt to access with different user_id should fail
    with pytest.raises(Exception):  # Should raise HTTPException
        store1.validate_user_access(user2_id)

@pytest.mark.asyncio
async def test_usage_tracking_per_user():
    """Test that usage is tracked per user"""
    user1_id = "test-user-1"
    user2_id = "test-user-2"
    
    # Log usage for different users
    await log_usage(user1_id, "chat", {"test": True})
    await log_usage(user2_id, "chat", {"test": True})
    
    # Check limits are enforced per user
    user1_can_chat = await check_usage_limits(user1_id, "chat")
    user2_can_chat = await check_usage_limits(user2_id, "chat")
    
    # Both should be able to chat (assuming under limits)
    assert user1_can_chat == True
    assert user2_can_chat == True

### **File: `tests/test_audit_logging.py` (NEW FILE)**

**CREATE AUDIT LOGGING TESTS (Requirement 7):**
```python
"""
Audit logging tests - CRITICAL for compliance
"""

import pytest
import asyncio
from server.services.usage_tracking import log_audit_action, log_tier_change, get_audit_logs

@pytest.mark.asyncio
async def test_audit_tier_change():
    """Test that tier changes are audited"""
    admin_id = "admin-user-1"
    user_id = "test-user-1"
    
    # Log tier change
    result = await log_tier_change(admin_id, user_id, "free", "pro")
    assert result == True

@pytest.mark.asyncio
async def test_audit_log_retrieval():
    """Test that audit logs can be retrieved by admins only"""
    admin_id = "admin-user-1"
    
    # This should work for admin users
    logs = await get_audit_logs(admin_id)
    assert isinstance(logs, list)

@pytest.mark.asyncio
async def test_audit_log_access_denied():
    """Test that non-admins cannot access audit logs"""
    regular_user_id = "regular-user-1"
    
    # This should fail for regular users
    with pytest.raises(Exception):  # Should raise HTTPException
        await get_audit_logs(regular_user_id)
```

---

## ‚úÖ **DEPLOYMENT CHECKLIST FOR KIRO**

### **CRITICAL VERIFICATION STEPS:**

1. **Authentication Fix (Requirement 1):**
   - [ ] Mock user code removed from auth.py line 142
   - [ ] Proper token validation implemented
   - [ ] All endpoints return 401 without valid token
   - [ ] Token expiry checking implemented
   - [ ] Session invalidation working
   - [ ] Last login timestamp updated

2. **User Isolation (Requirement 2):**
   - [ ] Vector store accepts user_id parameter
   - [ ] Collections named with user_{user_id}_docs pattern
   - [ ] Cross-user access validation implemented
   - [ ] User isolation tests passing
   - [ ] No cross-user data leakage possible

3. **Usage Tracking (Requirement 3):**
   - [ ] usage_tracking.py service created
   - [ ] All endpoints check limits before processing
   - [ ] Usage logged after successful operations
   - [ ] Tier limits properly enforced
   - [ ] 429 responses for exceeded limits
   - [ ] Accurate usage counts in user profiles

4. **Production Configuration (Requirement 4):**
   - [ ] Debug mode disabled
   - [ ] Production CORS settings applied
   - [ ] Per-user rate limiting implemented
   - [ ] Rate limiting middleware active
   - [ ] Production environment variables set
   - [ ] Error logging without sensitive data exposure

5. **Database Schema (Requirement 5):**
   - [ ] usage_logs table created with all required fields
   - [ ] user_documents table with soft-delete capability
   - [ ] audit_logs table for administrative actions
   - [ ] rate_limits table for rate limiting state
   - [ ] All indexes applied for performance
   - [ ] RLS policies enabled for data isolation
   - [ ] Foreign key constraints with cascade deletes

6. **Security Testing (Requirement 6):**
   - [ ] All security tests pass
   - [ ] User isolation tests pass
   - [ ] Usage limit tests pass
   - [ ] Rate limiting tests pass
   - [ ] No mock users in responses
   - [ ] Authentication required for all endpoints
   - [ ] Automated regression tests configured in CI/CD

7. **Audit Logging (Requirement 7):**
   - [ ] Audit logging functions implemented
   - [ ] Tier changes logged to audit_logs table
   - [ ] Document removals logged with metadata
   - [ ] Audit logs accessible only to admins
   - [ ] Audit log retention configured
   - [ ] All privileged operations logged

### **FINAL VALIDATION COMMAND:**
```bash
# Run this to verify all patches are working
python tests/test_security_patches.py
```

**üö® DO NOT DEPLOY until all checklist items are verified! üö®**

